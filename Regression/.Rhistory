best.gene<-names(geness)
sprintf("The gene I chose is: %s", best.gene)
plot(X[,best.gene], Time)
h <- 0.5
"Through trial and error I found h is roughly:"
h
fossil.loc <- locpoly(X[,best.gene], Time, bandwidth=h)
lines(fossil.loc, col=2, main = "Time vs best gene (G4248)")
plot(X[,best.gene], Time, main = "Time vs best gene (G4248)")
# ...
require(KernSmooth)
"I will pick one of the genes that has the lowest p value as it is likey a significant gene/feature for predicing the Response."
pvals<-summary(gene.lm.fit)$coef[-1,4]
geness<- which.min(pvals)
best.gene<-names(geness)
sprintf("The gene I chose is: %s", best.gene)
plot(X[,best.gene], Time, main = "Time vs best gene (G4248)")
h <- 0.5
"Through trial and error I found h is roughly:"
h
fossil.loc <- locpoly(X[,best.gene], Time, bandwidth=h)
lines(fossil.loc, col=2)
plot(X[,best.gene], Time, main = "Time vs best gene (G4248) fitted with locpoly")
# ...
require(KernSmooth)
"I will pick one of the genes that has the lowest p value as it is likey a significant gene/feature for predicing the Response."
pvals<-summary(gene.lm.fit)$coef[-1,4]
geness<- which.min(pvals)
best.gene<-names(geness)
sprintf("The gene I chose is: %s", best.gene)
plot(X[,best.gene], Time, main = "Time vs best gene (G4248) fitted with locpoly")
h <- 0.5
"Through trial and error I found h is roughly:"
h
fossil.loc <- locpoly(X[,best.gene], Time, bandwidth=h)
lines(fossil.loc, col=2)
# ...
smooth.loc<- function(x,y,xgrid=x, h){
N<-length(xgrid)
smooth.est<-rep(0,N)
for (j in 1:N){
smooth.est[j]<- lm(y~I(x-xgrid[j]),
weights=dnorm(x,xgrid[j],h) )$coef[1]
}
list(x= xgrid, fit=smooth.est)
}
h <- dpill(X[,best.gene], Time, truncate=FALSE)
Gene.loc <- smooth.loc(X[,best.gene], Time, h=h)
Gene.res <- Time - Gene.loc$fit
boot.fit <- matrix(0,200,dim(patient.data$x)[2])
for (j in 1:200){
boot.res <- sample(Gene.res, size=dim(patient.data$x)[2])
new.y <- Gene.loc$fit +boot.res
##### h selection
h <- dpill(X[,best.gene], new.y, truncate=FALSE)
boot.fit[j,] <- smooth.loc(X[,best.gene], new.y, h=h)$fit
}
"Scree plot"
pc.cr <- X
screeplot(pc.cr,xlab= "Principal components", main = "SCREE plot")
"Scree plot"
pc.cr <-  princomp(X, scale. = TRUE)
"Scree plot"
pc.cr <- prcomp(X, scale. = TRUE)
screeplot(pc.cr,xlab= "Principal components", main = "SCREE plot")
"Scree plot"
pc.cr <- prcomp(X, scale. = TRUE, npcs = 4)
screeplot(pc.cr,xlab= "Principal components", main = "SCREE plot")
"Scree plot"
pc.cr <- prcomp(X,  npcs = 4,scale. = TRUE)
screeplot(pc.cr,xlab= "Principal components", main = "SCREE plot")
"Scree plot"
pc.cr <- prcomp(X,  npcs = 4,scale. = TRUE,main = deparse1(substitute(x)))
screeplot(pc.cr,xlab= "Principal components", main = "SCREE plot")
"Scree plot"
pc.cr <- prcomp(X,  npcs = 4,scale. = TRUE,main = deparse1(substitute(x)),type = "lines")
screeplot(pc.cr,xlab= "Principal components", main = "SCREE plot")
"Scree plot"
pc.cr <- prcomp(X, scale. = TRUE)
screeplot(pc.cr, npcs = 4,xlab= "Principal components", main = "SCREE plot", ,main = deparse1(substitute(x)),type = "lines")
"Scree plot"
pc.cr <- prcomp(X, scale. = TRUE)
screeplot(pc.cr, npcs = 4,xlab= "Principal components", main = "SCREE plot", main = deparse1(substitute(x)),type = "lines")
"Scree plot"
pc.cr <- prcomp(X, scale. = TRUE)
screeplot(pc.cr, npcs = 4,xlab= "Principal components", main = "SCREE plot",type = "lines")
"Scree plot"
pc.cr <- prcomp(X, scale. = TRUE)
screeplot(pc.cr, npcs = 4, main = "SCREE plot",type = "lines")
"Scree plot"
pc.cr <- prcomp(X, scale. = TRUE)
screeplot(pc.cr, npcs = 4, main = "SCREE plot")
"Scree plot"
pc.cr <- prcomp(X, scale. = TRUE)
screeplot(pc.cr, npcs = 6, main = "SCREE plot")
"Scree plot"
pc.cr <- prcomp(X, scale. = TRUE)
screeplot(pc.cr, npcs = 8, main = "SCREE plot",type = "lines")
require(HCmodelSets)
data(LymphomaData)
?patient.data
names(patient.data)
dim(patient.data$x)
X <- t(patient.data$x)
colnames(X) <-paste("G", 1:dim(X)[2], sep="")
Time <- patient.data$time
# ...
"Exploring the response variable"
#Histogram of response
hist(Time)
#Mean
"Mean Time"
mean(Time)
"STD of Time"
sd(Time)
"sample quantiles of Time"
quantile(Time)
"Number of missing values in Time"
sum(is.na(Time))
"Exploring the Genes (X) variable, Cant explore all as so many so use a random sample of 9 genes"
""
xxx <- 1:7399
ind<-c(sample(xxx,9))
DataExplorer::plot_histogram(X[,ind], ncol = 3)
#Mean
"Mean X"
colMeans(X[,ind])
"STD of X"
apply(X[,ind],2,sd)
"sample quantiles of X"
apply(X[,ind],2,quantile)
"Number of missing values in X"
sum(is.na(X[,ind]))
"Scree plot"
pc.cr <- prcomp(X, scale. = TRUE)
screeplot(pc.cr, npcs = 8, main = "SCREE plot",type = "lines")
"Pairs plot of 3 random genes with time"
xxx <- 1:7399
ind<-c(sample(xxx,3))
A = cbind(X[,ind], Time)
pairs(A)
"Exploring the Status variable"
"Counts of the status:"
table(patient.data$status)
"NOTE I DID SCLAE THE DATA AND RUN THE CELLS BELOW, THE RESULTS DID NOT CHANGE THEREFORE i DID NOT USE SCALING IN THE END"
Unscale <- function(X,OG) {
std <- apply(OG, 2, sd)
meen <- colMeans(OG)
XX <-sweep(X,2,std,`*`)
scaled <- sweep(XX, 2, meen, `+`)
return(scaled)
}
Scale <- function(X) {
std <- apply(X, 2, sd)
meen <- colMeans(X)
XX <- sweep(X, 2, meen, `-`)
X <-sweep(XX,2,std,"/")
return(X)
}
# ...
require(glmnet)
gene.lasso<- glmnet(X, Time, family="gaussian", alpha=1)
par(mar = c(6,4,4,2))
plot(gene.lasso, xvar="lambda")
title("trace plot of the fitted regression coefficients", line = +3)
gene.lasso.cv= cv.glmnet(X, Time, alpha=1 )
par(mfrow=c(1,1), mai=c(0.8,0.8,0.8,0.8), cex.axis=2)
plot(gene.lasso.cv)
title("MSE againts log(lambda)", line = +3)
gene.lasso.cv$lambda.min
"Statement"
""
"Firstly from the Trace plot we can clearly see as lamdba approaches zero the loss fucntion of the lasso tends to the OLS function. Therefore as lambda grows we get stonger reguilarisation
The lambda which *minimizes* the cross-validation criterion was a small -ve number (~ -0.7 , dont wont to specify exact value as it changes slighly between runs) therefore we are using
a fairly strong regularisation.Hinting that we do not need many of these ~7000 genes as there are a small subset of genes that seem to capture a most of the information. This is why we see
most of the coefficients tedning towards zero as we approach log(lambda) = 0.I.E most of the genes/features here are not useful/needed.
The MSE vs Lambda allows us to find the best lambda that minismises the MSE using cross validation. So overall we find the best lamda through minimising MSE through CV. We can interprete this result using the trace plot, as at this lamdba on the trace plot we can see how many coefficients are non zero i.e important coefficients that help us predict the response variable (Time). We can even get a rough idea of the magnitude of these coeficients.We can also see from the MSE vs Lambda graph that the number of params needed is roughly between 5-36"
""
gene.lasso.min_lambda <- gene.lasso.cv$lambda.min
"lambda that minimises the cross-validation criterion"
gene.lasso.min_lambda
gene.lasso.coef <- coef(gene.lasso.cv, s="lambda.min")
apply(gene.lasso.coef, 2, function(c)sum(c!=0))
sprintf("The number of parameters is = %d", apply(gene.lasso.coef, 2, function(c)sum(c!=0)))
"The names are:"
as.vector(rownames(gene.lasso.coef)[gene.lasso.coef[,1] != 0])
# ...
lambda_vals <- c()
row_names <- c()
require(glmnet)
for(i in 1:50){
gene.lasso.cv= cv.glmnet(X, Time, alpha=1 )
gene.lasso.min_lambda <- gene.lasso.cv$lambda.min
lambda_vals[i] <- gene.lasso.min_lambda
gene.lasso.coef <- coef(gene.lasso.cv, s="lambda.min")
z <- as.vector(rownames(gene.lasso.coef)[gene.lasso.coef[,1] != 0])
row_names <- c(row_names,z)
print(i)
}
"All selected params"
result<- as.data.frame(table(row_names))
result
""
"All Params with counts >= 25"
data <- result[result[,2] >= 25,]
data
Selected_var <- as.vector(data[,1])
b <- paste(Selected_var[-1], collapse="+")
b
gene.lm.fit <- lm(paste("Time~ ",b,sep = "")	, data = as.data.frame(X))
summary(gene.lm.fit)
"Selected genes:"
Selected_var[-1]
# ...
par(mfrow=c(1,2), cex=0.6)
plot(gene.lm.fit$residuals, main = "Residuals before transform")
plot(gene.lm.fit$fitted, gene.lm.fit$residuals, main = "Residuals vs fitted before transform")
require(MASS)
boxcox(gene.lm.fit, main = "BoxCox Plot")
"Since lambda = 0 we say y^(lambda) = log(y)"
Trans_fit <- lm(paste("log(Time) ~ ",b,sep = "")	, data = as.data.frame(X))
summary(Trans_fit)
par(mfrow=c(1,2), cex=0.6)
plot(Trans_fit$residuals, main = "Residuals after transform")
plot(Trans_fit$fitted, Trans_fit$residuals, main = "Residuals vs fitted after transform")
"Original r squared before transform:"
summary(gene.lm.fit)$r.squared
""
"New r squared after transform:"
summary(Trans_fit)$r.squared
"Lets start by looking at the resiual analysis:
Residuals plot: before and after the transform the residuals look independent due to no pattern.
Fitted residuals plot: Before the transform clear pattern (trumpet like) which suggests we have violated homoscedasticity
After the transform we see no such shapes. suggesting the boxcox transform has not violated this. From this I am siding with the teansform, but lets compare r squared values.
From the r squared values we can clearly see the non-transformed model has the best statistic
So from this I have had to look else where for a better answer, the model:
As we are using ordinary least squares which assumes normality I will have to say I would prefer to use the transform model as it 'garantees' this wont be violated and we can see it fixes the problem of the data violating homoscedasticity, the only caveat is the transform model has a slighly worse r squared stat."
# ...
require(KernSmooth)
"I will pick one of the genes that has the lowest p value as it is likey a significant gene/feature for predicing the Response."
pvals<-summary(gene.lm.fit)$coef[-1,4]
geness<- which.min(pvals)
best.gene<-names(geness)
sprintf("The gene I chose is: %s", best.gene)
plot(X[,best.gene], Time, main = "Time vs best gene (G4248) fitted with locpoly")
h <- 0.5
"Through trial and error I found h is roughly:"
h
fossil.loc <- locpoly(X[,best.gene], Time, bandwidth=h)
lines(fossil.loc, col=2)
# ...
smooth.loc<- function(x,y,xgrid=x, h){
N<-length(xgrid)
smooth.est<-rep(0,N)
for (j in 1:N){
smooth.est[j]<- lm(y~I(x-xgrid[j]),
weights=dnorm(x,xgrid[j],h) )$coef[1]
}
list(x= xgrid, fit=smooth.est)
}
h <- dpill(X[,best.gene], Time, truncate=FALSE)
Gene.loc <- smooth.loc(X[,best.gene], Time, h=h)
Gene.res <- Time - Gene.loc$fit
boot.fit <- matrix(0,200,dim(patient.data$x)[2])
for (j in 1:200){
boot.res <- sample(Gene.res, size=dim(patient.data$x)[2])
new.y <- Gene.loc$fit +boot.res
##### h selection
h <- dpill(X[,best.gene], new.y, truncate=FALSE)
boot.fit[j,] <- smooth.loc(X[,best.gene], new.y, h=h)$fit
}
lower<-upper<- rep(0,dim(patient.data$x)[2])
for (i in 1:dim(patient.data$x)[2]){
lower[i]<-quantile(boot.fit[,i],0.025)
upper[i]<-quantile(boot.fit[,i],0.975)
}
plot(X[,best.gene], Time, main = "Time vs best gene (G4248) using nonparametric smoother and confidence bands")
for (j in 1:200){ lines(X[,best.gene][order(X[,best.gene])], boot.fit[j,][order(X[,best.gene])], col="grey")}
lines(X[,best.gene][order(X[,best.gene])], Gene.loc$fit[order(X[,best.gene])])
lines(X[,best.gene][order(X[,best.gene])], upper[order(X[,best.gene])], col=2, lwd=3)
lines(X[,best.gene][order(X[,best.gene])], lower[order(X[,best.gene])], col=2, lwd=3)
tt<-lm(Time ~ X[,best.gene], data = as.data.frame(X))
abline(tt, col=5, lwd=2)
"R squared of parametric model"
summary(tt)$r.squared
""
"R squared of non-parametric model"
rr<- 1-(sum((Time-Gene.loc$fit)^2))/(sum((Time-mean(Time))^2))
rr
"statement on the usefulness of the nonparametric model:
As you can see the non parametric model had a better r squared and therfore fitted the data better, non parametric models are also very useful as they usaully rquire less tuning in terms of hyperparamters. They are also a very general model which can be used in manyu applications.In this case they out performed the paramteric models making them very useful and easy to use!"
"I used the dpill to select my smoothing paramter, This simply finds the bandwidth h which minimises the integrated asymptotic mean squared error where h ~ n^-1/5"
# ...
smooth.loc<- function(x,y,xgrid=x, h){
N<-length(xgrid)
smooth.est<-rep(0,N)
for (j in 1:N){
smooth.est[j]<- lm(y~I(x-xgrid[j]),
weights=dnorm(x,xgrid[j],h) )$coef[1]
}
list(x= xgrid, fit=smooth.est)
}
h <- dpill(X[,best.gene], Time, truncate=FALSE)
Gene.loc <- smooth.loc(X[,best.gene], Time, h=h)
Gene.res <- Time - Gene.loc$fit
boot.fit <- matrix(0,200,dim(patient.data$x)[2])
for (j in 1:200){
boot.res <- sample(Gene.res, size=dim(patient.data$x)[2])
new.y <- Gene.loc$fit +boot.res
##### h selection
h <- dpill(X[,best.gene], new.y, truncate=FALSE)
boot.fit[j,] <- smooth.loc(X[,best.gene], new.y, h=h)$fit
}
require(HCmodelSets)
data(LymphomaData)
?patient.data
names(patient.data)
dim(patient.data$x)
X <- t(patient.data$x)
colnames(X) <-paste("G", 1:dim(X)[2], sep="")
Time <- patient.data$time
# ...
"Exploring the response variable"
#Histogram of response
hist(Time)
#Mean
"Mean Time"
mean(Time)
"STD of Time"
sd(Time)
"sample quantiles of Time"
quantile(Time)
"Number of missing values in Time"
sum(is.na(Time))
"Exploring the Genes (X) variable, Cant explore all as so many so use a random sample of 9 genes"
""
xxx <- 1:7399
ind<-c(sample(xxx,9))
DataExplorer::plot_histogram(X[,ind], ncol = 3)
#Mean
"Mean X"
colMeans(X[,ind])
"STD of X"
apply(X[,ind],2,sd)
"sample quantiles of X"
apply(X[,ind],2,quantile)
"Number of missing values in X"
sum(is.na(X[,ind]))
"Scree plot"
pc.cr <- prcomp(X, scale. = TRUE)
screeplot(pc.cr, npcs = 8, main = "SCREE plot",type = "lines")
"Pairs plot of 3 random genes with time"
xxx <- 1:7399
ind<-c(sample(xxx,3))
A = cbind(X[,ind], Time)
pairs(A)
"Exploring the Status variable"
"Counts of the status:"
table(patient.data$status)
"NOTE I DID SCLAE THE DATA AND RUN THE CELLS BELOW, THE RESULTS DID NOT CHANGE THEREFORE i DID NOT USE SCALING IN THE END"
Unscale <- function(X,OG) {
std <- apply(OG, 2, sd)
meen <- colMeans(OG)
XX <-sweep(X,2,std,`*`)
scaled <- sweep(XX, 2, meen, `+`)
return(scaled)
}
Scale <- function(X) {
std <- apply(X, 2, sd)
meen <- colMeans(X)
XX <- sweep(X, 2, meen, `-`)
X <-sweep(XX,2,std,"/")
return(X)
}
# ...
require(glmnet)
gene.lasso<- glmnet(X, Time, family="gaussian", alpha=1)
par(mar = c(6,4,4,2))
plot(gene.lasso, xvar="lambda")
title("trace plot of the fitted regression coefficients", line = +3)
gene.lasso.cv= cv.glmnet(X, Time, alpha=1 )
par(mfrow=c(1,1), mai=c(0.8,0.8,0.8,0.8), cex.axis=2)
plot(gene.lasso.cv)
title("MSE againts log(lambda)", line = +3)
gene.lasso.cv$lambda.min
"Statement"
""
"Firstly from the Trace plot we can clearly see as lamdba approaches zero the loss fucntion of the lasso tends to the OLS function. Therefore as lambda grows we get stonger reguilarisation
The lambda which *minimizes* the cross-validation criterion was a small -ve number (~ -0.7 , dont wont to specify exact value as it changes slighly between runs) therefore we are using
a fairly strong regularisation.Hinting that we do not need many of these ~7000 genes as there are a small subset of genes that seem to capture a most of the information. This is why we see
most of the coefficients tedning towards zero as we approach log(lambda) = 0.I.E most of the genes/features here are not useful/needed.
The MSE vs Lambda allows us to find the best lambda that minismises the MSE using cross validation. So overall we find the best lamda through minimising MSE through CV. We can interprete this result using the trace plot, as at this lamdba on the trace plot we can see how many coefficients are non zero i.e important coefficients that help us predict the response variable (Time). We can even get a rough idea of the magnitude of these coeficients.We can also see from the MSE vs Lambda graph that the number of params needed is roughly between 5-36"
""
gene.lasso.min_lambda <- gene.lasso.cv$lambda.min
"lambda that minimises the cross-validation criterion"
gene.lasso.min_lambda
gene.lasso.coef <- coef(gene.lasso.cv, s="lambda.min")
apply(gene.lasso.coef, 2, function(c)sum(c!=0))
sprintf("The number of parameters is = %d", apply(gene.lasso.coef, 2, function(c)sum(c!=0)))
"The names are:"
as.vector(rownames(gene.lasso.coef)[gene.lasso.coef[,1] != 0])
# ...
lambda_vals <- c()
row_names <- c()
require(glmnet)
for(i in 1:50){
gene.lasso.cv= cv.glmnet(X, Time, alpha=1 )
gene.lasso.min_lambda <- gene.lasso.cv$lambda.min
lambda_vals[i] <- gene.lasso.min_lambda
gene.lasso.coef <- coef(gene.lasso.cv, s="lambda.min")
z <- as.vector(rownames(gene.lasso.coef)[gene.lasso.coef[,1] != 0])
row_names <- c(row_names,z)
print(i)
}
"All selected params"
result<- as.data.frame(table(row_names))
result
""
"All Params with counts >= 25"
data <- result[result[,2] >= 25,]
data
Selected_var <- as.vector(data[,1])
b <- paste(Selected_var[-1], collapse="+")
b
gene.lm.fit <- lm(paste("Time~ ",b,sep = "")	, data = as.data.frame(X))
summary(gene.lm.fit)
"Selected genes:"
Selected_var[-1]
# ...
par(mfrow=c(1,2), cex=0.6)
plot(gene.lm.fit$residuals, main = "Residuals before transform")
plot(gene.lm.fit$fitted, gene.lm.fit$residuals, main = "Residuals vs fitted before transform")
require(MASS)
boxcox(gene.lm.fit, main = "BoxCox Plot")
"Since lambda = 0 we say y^(lambda) = log(y)"
Trans_fit <- lm(paste("log(Time) ~ ",b,sep = "")	, data = as.data.frame(X))
summary(Trans_fit)
par(mfrow=c(1,2), cex=0.6)
plot(Trans_fit$residuals, main = "Residuals after transform")
plot(Trans_fit$fitted, Trans_fit$residuals, main = "Residuals vs fitted after transform")
"Original r squared before transform:"
summary(gene.lm.fit)$r.squared
""
"New r squared after transform:"
summary(Trans_fit)$r.squared
"Lets start by looking at the resiual analysis:
Residuals plot: before and after the transform the residuals look independent due to no pattern.
Fitted residuals plot: Before the transform clear pattern (trumpet like) which suggests we have violated homoscedasticity
After the transform we see no such shapes. suggesting the boxcox transform has not violated this. From this I am siding with the teansform, but lets compare r squared values.
From the r squared values we can clearly see the non-transformed model has the best statistic
So from this I have had to look else where for a better answer, the model:
As we are using ordinary least squares which assumes normality I will have to say I would prefer to use the transform model as it 'garantees' this wont be violated and we can see it fixes the problem of the data violating homoscedasticity, the only caveat is the transform model has a slighly worse r squared stat."
# ...
require(KernSmooth)
"I will pick one of the genes that has the lowest p value as it is likey a significant gene/feature for predicing the Response."
pvals<-summary(gene.lm.fit)$coef[-1,4]
geness<- which.min(pvals)
best.gene<-names(geness)
sprintf("The gene I chose is: %s", best.gene)
plot(X[,best.gene], Time, main = "Time vs best gene (G4248) fitted with locpoly")
h <- 0.5
"Through trial and error I found h is roughly:"
h
fossil.loc <- locpoly(X[,best.gene], Time, bandwidth=h)
lines(fossil.loc, col=2)
# ...
smooth.loc<- function(x,y,xgrid=x, h){
N<-length(xgrid)
smooth.est<-rep(0,N)
for (j in 1:N){
smooth.est[j]<- lm(y~I(x-xgrid[j]),
weights=dnorm(x,xgrid[j],h) )$coef[1]
}
list(x= xgrid, fit=smooth.est)
}
h <- dpill(X[,best.gene], Time, truncate=FALSE)
Gene.loc <- smooth.loc(X[,best.gene], Time, h=h)
Gene.res <- Time - Gene.loc$fit
boot.fit <- matrix(0,200,dim(patient.data$x)[2])
for (j in 1:200){
boot.res <- sample(Gene.res, size=dim(patient.data$x)[2])
new.y <- Gene.loc$fit +boot.res
##### h selection
h <- dpill(X[,best.gene], new.y, truncate=FALSE)
boot.fit[j,] <- smooth.loc(X[,best.gene], new.y, h=h)$fit
}
lower<-upper<- rep(0,dim(patient.data$x)[2])
for (i in 1:dim(patient.data$x)[2]){
lower[i]<-quantile(boot.fit[,i],0.025)
upper[i]<-quantile(boot.fit[,i],0.975)
}
plot(X[,best.gene], Time, main = "Time vs best gene (G4248) using nonparametric smoother and confidence bands")
for (j in 1:200){ lines(X[,best.gene][order(X[,best.gene])], boot.fit[j,][order(X[,best.gene])], col="grey")}
lines(X[,best.gene][order(X[,best.gene])], Gene.loc$fit[order(X[,best.gene])])
lines(X[,best.gene][order(X[,best.gene])], upper[order(X[,best.gene])], col=2, lwd=3)
lines(X[,best.gene][order(X[,best.gene])], lower[order(X[,best.gene])], col=2, lwd=3)
tt<-lm(Time ~ X[,best.gene], data = as.data.frame(X))
abline(tt, col=5, lwd=2)
"R squared of parametric model"
summary(tt)$r.squared
""
"R squared of non-parametric model"
rr<- 1-(sum((Time-Gene.loc$fit)^2))/(sum((Time-mean(Time))^2))
rr
"statement on the usefulness of the nonparametric model:
As you can see the non parametric model had a better r squared and therfore fitted the data better, non parametric models are also very useful as they usaully rquire less tuning in terms of hyperparamters. They are also a very general model which can be used in manyu applications.In this case they out performed the paramteric models making them very useful and easy to use! However only uing one of the genes to predict our response is not useful in either case and we get very poor r squared values."
"I used the dpill to select my smoothing paramter, This simply finds the bandwidth h which minimises the integrated asymptotic mean squared error where h ~ n^-1/5"
plot(cars)
hotels <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv")
View(hotels)
hotels <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv")
View(hotels)
